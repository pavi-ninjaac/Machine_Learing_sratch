{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from supervised.decision_tree import DecisionTreeClassifier, DecisionTreeRegression\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "class RandomForest:\n",
    "    \"\"\"    Random forest common class.    \"\"\"\n",
    "    def __init__(self, trees, n_trees, max_feature ,\n",
    "                 prediction_aggrigation_calculation):\n",
    "        \"\"\"\n",
    "        :param trees: List - list of tree objects. classification tree/regression trees.\n",
    "        :param n_trees: int - How may estimators/tree should be used for random forest building.\n",
    "        :param max_feature: Int - How many features can be used for a tree from the whole features.\n",
    "        :param prediction_aggrigation_calculation: Function - Aggication function to find the prediction.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_trees\n",
    "        self.max_features = max_feature\n",
    "        self.tree_feature_indexes = []\n",
    "        self.prediction_aggrigation_calculation = prediction_aggrigation_calculation \n",
    "        # Initialize the trees.\n",
    "        self.trees = trees\n",
    "\n",
    "    def _make_random_suset(self, X, y, n_subsets, replasment=True):\n",
    "        \"\"\"\n",
    "        Creata a random subset of dataset with/without replacement.\n",
    "\n",
    "        :param X: Depentand variables.\n",
    "        :param y: Indepentant variable.\n",
    "        :param n_subsets: Number of subset we need.\n",
    "        :param replasment: Boolena - Can we use the data sample again or not.\n",
    "        \"\"\"\n",
    "        subset = []\n",
    "        # use 100% of data when replacement is true , use 50% otherwise.\n",
    "        sample_size = (X.shape[0] if replasment else (X.shape[0] // 2))\n",
    "\n",
    "        # First concadinate the X and y datasets in order to make a choice.\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        np.random.shuffle(Xy)\n",
    "        # Select randome subset of data with replacement.\n",
    "        for i in range(n_subsets):\n",
    "            index = np.random.choice(range(sample_size), size=np.shape(range(sample_size)), replace=replasment)\n",
    "            X = Xy[index][:, :-1]\n",
    "            y = Xy[index][: , -1]\n",
    "            subset.append({\"X\" : X, \"y\": y})\n",
    "        return subset\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the model.\n",
    "        :param X: Depentand variables.\n",
    "        :param y: Indepentant variable.\n",
    "        \"\"\"\n",
    "        # if the max_features is not given then select it as square root of no on feature availabe.\n",
    "        n_features = X.shape[1]\n",
    "        if self.max_features == None:\n",
    "            self.max_features = int(math.sqrt(n_features))\n",
    "\n",
    "        # Split the dataset into number of subsets equal to n_estimators.\n",
    "        subsets = self._make_random_suset(X, y, self.n_estimators)\n",
    "\n",
    "        for i, subset in enumerate(subsets):\n",
    "            X_subset , y_subset = subset[\"X\"], subset[\"y\"]\n",
    "            # select a random sucset of features for each tree. This is called feature bagging.\n",
    "            idx = np.random.choice(range(n_features), size=self.max_features, replace=True)\n",
    "            # track this for prediction.\n",
    "            self.tree_feature_indexes.append(idx)\n",
    "            # Get the X with the selected features only.\n",
    "            X_subset = X_subset[:, idx]\n",
    "\n",
    "            # change the y_subet to i dimentional array.\n",
    "            y_subset = np.expand_dims(y_subset, axis =1)\n",
    "            # build the model with selected features and selected random subset from dataset.\n",
    "            self.trees[i].train(X_subset, y_subset)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        Predict the new samples.\n",
    "\n",
    "        :param test_X: Depentant variables for prediction.\n",
    "        \"\"\"\n",
    "        # predict each sample one by one.\n",
    "        y_preds = np.empty((test_X.shape[0], self.n_estimators))\n",
    "        # find the prediction from each tree for eeach samples\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            features_index = self.tree_feature_indexes[i]\n",
    "            X_selected_features = test_X[:, features_index]\n",
    "            if isinstance(tree, DecisionTreeClassifier):\n",
    "                y_preds[:, i] = tree.predict(X_selected_features).reshape((-1,))\n",
    "            else:\n",
    "                y_preds[:, i] = tree.predict(X_selected_features)\n",
    "        # find the arrgrecated output.\n",
    "        y_pred = self.prediction_aggrigation_calculation(y_preds)\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random forest Classifier from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "class RandomForestClassifier(RandomForest):\n",
    "    \"\"\"Rnadom forest for classification task.\"\"\"\n",
    "    def __init__(self, max_feature, max_depth, n_trees=100, min_sample_split=2, min_impurity=1e-7):\n",
    "        \"\"\"\n",
    "        :param max_depth: Int - Max depth of each tree.\n",
    "        :param n_trees: Int - Number of trees/estimetors.\n",
    "        :param min_sample_split: Int - minimum samples for a node to have before going for split.\n",
    "        :param min_impurity: Int - Min inpurity a node can have.\n",
    "        \"\"\"\n",
    "        self.prediction_aggrigation_calculation = self._maximum_vote_calculation\n",
    "        \n",
    "        # Initializing the trees.\n",
    "        self.trees = []\n",
    "        for _ in range(n_trees):\n",
    "            self.trees.append(DecisionTreeClassifier(min_sample_split=min_sample_split,\n",
    "                                                     min_impurity=min_impurity, max_depth=max_depth))\n",
    "\n",
    "        super().__init__(trees=self.trees, n_trees=n_trees,max_feature=max_feature,\n",
    "                         prediction_aggrigation_calculation=self.prediction_aggrigation_calculation)\n",
    "    \n",
    "    def _maximum_vote_calculation(self, y_preds):\n",
    "        \"\"\"\n",
    "        Find which prediction class has higest frequency in all tree prediction for each sampple.\n",
    "\n",
    "        :param y_preds: Prediction value from number of estimators trees.\n",
    "        \"\"\"\n",
    "        # create a empty array to store the prediction.\n",
    "        y_pred = np.empty((y_preds.shape[0], 1))\n",
    "        # iterate over all the data samples.\n",
    "        for i, sample_predictions in enumerate(y_preds):\n",
    "            y_pred[i] = np.bincount(sample_predictions.astype('int')).argmax()\n",
    "\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "# X = np.array([[\"Green\", 3], [\"yello\", 3], [\"orange_color\", 2], [\"orange_color\", 2], [\"red\", 1]])\n",
    "# y = np.array([\"apply\", \"apply\", \"orange\", \"orange\", \"cherry\"])\n",
    "# X = pd.DataFrame(X)\n",
    "# y = pd.DataFrame(y)\n",
    "# y.head\n",
    "\n",
    "# Define the traning data.\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, n_features=5)\n",
    "\n",
    "# Chnage the shape of the target to 1 dimentional array.\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 1000\n",
      "Number of training features --------> 5\n",
      "Shape of the target value ----------> (1000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "# display the data.\n",
    "data = pd.DataFrame(X)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.215307 -0.660581 -1.350245 -0.231831 -0.398433\n",
       "1 -1.415503 -1.595183 -1.155783  0.973351 -0.314888\n",
       "2 -1.775128 -1.906776 -1.341324  1.192777  1.737915\n",
       "3 -0.373321 -0.699046 -0.625991  0.339499  0.711422\n",
       "4  0.603384  0.385183  0.152516 -0.327224 -1.461580"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.215307</td>\n",
       "      <td>-0.660581</td>\n",
       "      <td>-1.350245</td>\n",
       "      <td>-0.231831</td>\n",
       "      <td>-0.398433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.415503</td>\n",
       "      <td>-1.595183</td>\n",
       "      <td>-1.155783</td>\n",
       "      <td>0.973351</td>\n",
       "      <td>-0.314888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.775128</td>\n",
       "      <td>-1.906776</td>\n",
       "      <td>-1.341324</td>\n",
       "      <td>1.192777</td>\n",
       "      <td>1.737915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.373321</td>\n",
       "      <td>-0.699046</td>\n",
       "      <td>-0.625991</td>\n",
       "      <td>0.339499</td>\n",
       "      <td>0.711422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.603384</td>\n",
       "      <td>0.385183</td>\n",
       "      <td>0.152516</td>\n",
       "      <td>-0.327224</td>\n",
       "      <td>-1.461580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "# display the data.\n",
    "data_y = pd.DataFrame(y)\n",
    "data_y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "#define the parameters\n",
    "\n",
    "param = {\n",
    "    \"n_neibours\" : 5\n",
    "}\n",
    "print(\"=\"*100)\n",
    "random_forest_cla = RandomForestClassifier(n_trees=3, max_feature=2, min_sample_split=2, max_depth=45)\n",
    "\n",
    "# Train the model.\n",
    "random_forest_cla.train(X, y) \n",
    "\n",
    "# Predict the values.\n",
    "y_pred = random_forest_cla.predict(X)\n",
    "\n",
    "#calculate accuracy.\n",
    "acc = np.sum(y==y_pred)/X.shape[0]\n",
    "print(\"=\"*100)\n",
    "print(\"Accuracy of the prediction is {}\".format(acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Accuracy of the prediction is 0.965\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random forest Regression from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "class RandomForestRegression(RandomForest):\n",
    "    \"\"\"Rnadom forest for classification task.\"\"\"\n",
    "    def __init__(self, max_feature, max_depth, n_trees=100, min_sample_split=2, min_impurity=1e-7):\n",
    "        \"\"\"\n",
    "        :param max_depth: Int - Max depth of each tree.\n",
    "        :param n_trees: Int - Number of trees/estimetors.\n",
    "        :param min_sample_split: Int - minimum samples for a node to have before going for split.\n",
    "        :param min_impurity: Int - Min inpurity a node can have.\n",
    "        \"\"\"\n",
    "        self.prediction_aggrigation_calculation = self._mean_calculation\n",
    "        \n",
    "        # Initializing the trees.\n",
    "        self.trees = []\n",
    "        for _ in range(n_trees):\n",
    "            self.trees.append(DecisionTreeRegression(min_sample_split=min_sample_split,\n",
    "                                                     min_impurity=min_impurity, max_depth=max_depth))\n",
    "\n",
    "        super().__init__(trees=self.trees, n_trees=n_trees,max_feature=max_feature,\n",
    "                         prediction_aggrigation_calculation=self.prediction_aggrigation_calculation)\n",
    "    \n",
    "    def _mean_calculation(self, y_preds):\n",
    "        \"\"\"\n",
    "        Find mean prediction of all tree prediction for each sampple.\n",
    "\n",
    "        :param y_preds: Prediction value from number of estimators trees.\n",
    "        \"\"\"\n",
    "        # create a empty array to store the prediction.\n",
    "        y_pred = np.empty((y_preds.shape[0], 1))\n",
    "        # iterate over all the data samples.\n",
    "        for i, sample_predictions in enumerate(y_preds):\n",
    "            y_pred[i] = np.mean(sample_predictions)\n",
    "\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "# Define the traning data.\n",
    "X, y = make_regression(n_samples=1000, n_features=8)\n",
    "\n",
    "# Chnage the shape of the target to 1 dimentional array.\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 1000\n",
      "Number of training features --------> 8\n",
      "Shape of the target value ----------> (1000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# display the data.\n",
    "data_y = pd.DataFrame(y)\n",
    "data_y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            0\n",
       "0  261.258202\n",
       "1    0.393038\n",
       "2   32.156202\n",
       "3  128.667816\n",
       "4   -1.196097"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261.258202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.393038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.156202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128.667816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.196097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "#define the parameters\n",
    "print(\"=\"*100)\n",
    "random_forest_reg = RandomForestRegression(n_trees=3, max_feature=3, min_sample_split=2, max_depth=45)\n",
    "\n",
    "# Train the model.\n",
    "random_forest_reg.train(X, y) \n",
    "\n",
    "# Predict the values.\n",
    "y_pred = random_forest_reg.predict(X)\n",
    "#Root mean square error.\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "The r2_score of the trained model 0.7698232305798236\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decision Tree (scratch coding) - The much know Explanation :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Decision Trees** -->\n",
    " To put in simple words , It is a bunch of  if-else statements ordered in a binary tree format. Building the tree would start from the root node with whole training dataset. Each node either splitted into 2 branches namely true branch and false branch or will remain as a leaf node(under some condition which is when min_sample_split reached or when the node is pure). \n",
    "The split will happen based on a yes or no question(if-else statement) -->    for a single data sample if the answer is true then it will go under true branch else vice versa. \n",
    "That is pretty much about the over all process......😃 \n",
    "\n",
    "**Here what we need next is:**\n",
    "1. How can we measure a node is pure/not?\n",
    "2. How to choose/find a best question to split the data?\n",
    "Both the question have the same answer --> Based on the  **Impurity function **\n",
    "\n",
    "- For **Classification** the impurity function is ***Information Gain (IG)***\n",
    "- For **Regression** the impurity function is ***Variance reduction***\n",
    "\n",
    "**Answer:**\n",
    "1. If the impurity function is **Zero** then the node is pure (i.e. it has data from one single class on classification data)\n",
    "2. Select a question which has the highest impurity value.\n",
    "\n",
    "Let's learn about Impurity function -------\n",
    "\n",
    "- **Information Gain**\n",
    "\n",
    "Value will range from 0-1  ---> 0 means 100% pure --- 1 means 100% not pure. \n",
    "\n",
    "```IG = Entropy(y) - p * Entropy(true_branch_y) - (1 - p) * Entropy(false_branch_y) ```\n",
    "Here , \n",
    "p --> probability of true branch data. [ len(true_branch_y) / len(y)]\n",
    "\n",
    "`Entropy = - p1 * (log(p1) / log(2)) - p2 * (log(p2) / log(2)) - p3 * (log(p3) / log(3)) ....`\n",
    "\n",
    "p1, p2, p3 --> Probability of each class.\n",
    "\n",
    "- **Variance reduction**\n",
    "\n",
    "Value will range from 0-1.\n",
    "\n",
    "```VR = Variance(y) - (fraction * variance(true_branch_y) + (1 - fraction) * variance(false_branch_y))```\n",
    "Here , \n",
    "fraction --> probability of true branch data. [ len(true_branch_y) / len(y)]\n",
    "\n",
    "**Steps in summary:** 😀\n",
    "1. Start from the root node with whole training data.\n",
    "2. Find a best question to split the data for that node.\n",
    "    2.1 . iterate over all the features .\n",
    "    2.2 . Find the unique values in the branch. Iterate over all unique values.\n",
    "    2.3 . For each (feature , unique_value) --> this frames a question and find impurity funtion for this question.\n",
    "    2.4 . Find Impurity function values for all the (feature , unique_value) pair.\n",
    "    2.5 . Choose a pair which has highest impurity function value.\n",
    "3. Split the node's data into true branch and false branch based on this question.\n",
    "4. Repeat 2 & 3 .\n",
    "5. stop when min_sample_split(minimum data a node should have before the split) reached or impurity of a node has became zero.\n",
    "\n",
    "**Prediction**:\n",
    "\n",
    "Classification - traverse a data sample from the top of the tree to the bottom and assign the class which has maximum frequency in that leaf node as a prediction. \n",
    "\n",
    "Regression -  traverse a data sample from top of the tree to the bottom and assign the mean of target values in that leaf node as a prediction.\n",
    "\n",
    "why we should use Decision Trees:\n",
    "- Easy to build and interpret.\n",
    "- Missing values and outlier have less significance on prediction.\n",
    "- Training "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}