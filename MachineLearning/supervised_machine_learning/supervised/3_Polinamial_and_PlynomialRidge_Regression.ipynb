{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import make_regression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common Regression class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Regression:\n",
    "    def __init__(self, learning_rate, iteration, regularization):\n",
    "        \"\"\"\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model.\n",
    "        self.lr = learning_rate\n",
    "        self.it = iteration\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        :param y: Original target value.\n",
    "        :param y_pred: predicted target value.\n",
    "        \"\"\"\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "    \n",
    "    def hypothesis(self, weights, bias, X):\n",
    "        \"\"\"\n",
    "        :param weights: parameter value weight.\n",
    "        :param X: Training samples.\n",
    "        \"\"\"\n",
    "        return np.dot(X, weights) #+ bias\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Target value should be in the shape of (n, 1) not (n, ).\n",
    "        # So, this will check that and change the shape to (n, 1), if not.\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            # we need to change it to the 1 D array, not a list.\n",
    "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
    "            return \n",
    "        \n",
    "        # m is the number of training samples.\n",
    "        self.m = X.shape[0]\n",
    "        # n is the number of features.\n",
    "        self.n = X.shape[1]\n",
    "\n",
    "        # Set the initial weight.\n",
    "        self.w = np.zeros((self.n , 1))\n",
    "\n",
    "        # bias.\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.it+1):\n",
    "            # 1. Find the predicted value through the hypothesis.\n",
    "            # 2. Find the Cost function value.\n",
    "            # 3. Find the derivation of weights.\n",
    "            # 4. Apply Gradient Decent.\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            #print(\"iteration\",it)\n",
    "            #print(\"y predict value\",y_pred)\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            #print(\"Cost function\",cost)\n",
    "            # fin the derivative.\n",
    "            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "            #print(\"weights derivation\",dw)\n",
    "            #db = -(2 / self.m) * np.sum((y_pred - y))\n",
    "\n",
    "            # change the weight parameter.\n",
    "            self.w = self.w - self.lr * dw\n",
    "            #print(\"updated weights\",self.w)\n",
    "            #self.b = self.b - self.lr * db\n",
    "\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Define the traning data.\n",
    "X, y = make_regression(n_samples=50000, n_features=8)\n",
    "\n",
    "# Chnage the shape of the target to 1 dimentional array.\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 50000\n",
      "Number of training features --------> 8\n",
      "Shape of the target value ----------> (50000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# display the data.\n",
    "data = pd.DataFrame(X)\n",
    "data.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.857270 -1.073422 -0.085756 -1.114568 -0.283575  0.139347 -0.141859   \n",
       "1  0.910372 -0.958207 -0.518141 -0.730059  0.601583 -1.012509 -0.722968   \n",
       "2 -1.075336  0.878671  1.151659  0.227459 -1.815386  0.067162  0.383429   \n",
       "3 -0.159892 -2.346193  0.028133 -1.292739 -0.047946 -0.565455  0.291282   \n",
       "4 -1.502441 -0.293354  1.033080 -0.478827 -0.525721 -0.647145  0.714062   \n",
       "\n",
       "          7  \n",
       "0 -0.123514  \n",
       "1  1.511905  \n",
       "2 -1.900678  \n",
       "3 -0.632634  \n",
       "4 -1.624335  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.857270</td>\n",
       "      <td>-1.073422</td>\n",
       "      <td>-0.085756</td>\n",
       "      <td>-1.114568</td>\n",
       "      <td>-0.283575</td>\n",
       "      <td>0.139347</td>\n",
       "      <td>-0.141859</td>\n",
       "      <td>-0.123514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.910372</td>\n",
       "      <td>-0.958207</td>\n",
       "      <td>-0.518141</td>\n",
       "      <td>-0.730059</td>\n",
       "      <td>0.601583</td>\n",
       "      <td>-1.012509</td>\n",
       "      <td>-0.722968</td>\n",
       "      <td>1.511905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.075336</td>\n",
       "      <td>0.878671</td>\n",
       "      <td>1.151659</td>\n",
       "      <td>0.227459</td>\n",
       "      <td>-1.815386</td>\n",
       "      <td>0.067162</td>\n",
       "      <td>0.383429</td>\n",
       "      <td>-1.900678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.159892</td>\n",
       "      <td>-2.346193</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>-1.292739</td>\n",
       "      <td>-0.047946</td>\n",
       "      <td>-0.565455</td>\n",
       "      <td>0.291282</td>\n",
       "      <td>-0.632634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.502441</td>\n",
       "      <td>-0.293354</td>\n",
       "      <td>1.033080</td>\n",
       "      <td>-0.478827</td>\n",
       "      <td>-0.525721</td>\n",
       "      <td>-0.647145</td>\n",
       "      <td>0.714062</td>\n",
       "      <td>-1.624335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# display the data.\n",
    "data_y = pd.DataFrame(y)\n",
    "data_y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            0\n",
       "0 -143.605801\n",
       "1 -237.409740\n",
       "2  112.838794\n",
       "3 -266.666499\n",
       "4  -10.782274"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-143.605801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-237.409740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112.838794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-266.666499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-10.782274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Polynomial Regression from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def PolynomialFeature(X, degree):\n",
    "    \"\"\"\n",
    "    It is type of feature engineering ---> adding some more features based on the exisiting features \n",
    "    by squaring or cubing.\n",
    "    :param X: data need to be converted.\n",
    "    :param degree: int- The degree of the polynomial that the features X will be transformed to.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # get the index combinations.\n",
    "    combination = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
    "    combination_index = [index for obj in combination for index in obj]\n",
    "\n",
    "    # generate a empty array with new shape.\n",
    "    new_n_features = len(combination_index)\n",
    "    X_new = np.empty((n_samples, new_n_features))\n",
    "\n",
    "    for i, com_index  in enumerate(combination_index):\n",
    "        X_new[:, i] = np.prod(X[:, com_index], axis=1)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# Used for Polynomial Ridge regression.\n",
    "class l2_regularization:\n",
    "    \"\"\"Regularization used for Ridge Regression\"\"\"\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"This will be retuned when we call this class.\"\n",
    "        return self.lamda * np.sum(np.square(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        \"Derivation of the regulariozation function.\"\n",
    "        return self.lamda * 2 * (weights)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class PolynamialRegression(Regression):\n",
    "    \"\"\"\n",
    "    Polynomail Regression is also a type of non-linear regression with no regularization. \n",
    "    Before fitting the linear regression, the dependant variable is tranformed to some polynomail degree.\n",
    "    This is basincally transforming linear data to have some nonliniarity.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, iteration, degree):\n",
    "        \"\"\"\n",
    "        :param learning_rate: [range from 0 to infinity] the stpe distance used while doing gradiant decent.\n",
    "        :param iteration: int - Number of iteration to do.\n",
    "        :param degree: int - The degree of the polynomial that the feature transformed to.\n",
    "        \"\"\"\n",
    "        self.degree = degree\n",
    "        # No regularization here. So, making the regularization methods to return 0.\n",
    "        self.regularization = lambda x: 0\n",
    "        self.regularization.derivation = lambda x: 0\n",
    "        super().__init__(learning_rate, iteration, self.regularization)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # change the data to \n",
    "        X_poly = PolynomialFeature(X, degree=self.degree)\n",
    "        return super().train(X_poly, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        test_X_poly = PolynomialFeature(test_X, degree=self.degree)\n",
    "        return super().predict(test_X_poly)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#define the parameters\n",
    "param = {\n",
    "    \"degree\" : 2,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"iteration\" : 100,\n",
    "}\n",
    "print(\"=\"*100)\n",
    "polynomial_reg = PolynamialRegression(**param)\n",
    "\n",
    "# Train the model.\n",
    "polynomial_reg.train(X, y) \n",
    "\n",
    "# Predict the values.\n",
    "y_pred = polynomial_reg.predict(X)\n",
    "\n",
    "#Root mean square error.\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "The Cost function for the iteration 10----->2524.546198902789 :)\n",
      "The Cost function for the iteration 20----->313.8199639696676 :)\n",
      "The Cost function for the iteration 30----->39.17839267886082 :)\n",
      "The Cost function for the iteration 40----->4.916567388701627 :)\n",
      "The Cost function for the iteration 50----->0.6225340983364702 :)\n",
      "The Cost function for the iteration 60----->0.08070495018731812 :)\n",
      "The Cost function for the iteration 70----->0.011282742313695108 :)\n",
      "The Cost function for the iteration 80----->0.0019608909310563647 :)\n",
      "The Cost function for the iteration 90----->0.0005118599780978334 :)\n",
      "The Cost function for the iteration 100----->0.00019559828225020284 :)\n",
      "The r2_score of the trained model 0.9999999891242503\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Polynomial Regression using scikit-learn for comparision"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# data is already defined, going to use the same data for comparision.\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 50000\n",
      "Number of training features --------> 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "linear_reg_sklearn = LinearRegression()\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "X_new = poly.fit_transform(X)\n",
    "linear_reg_sklearn.fit(X, y)\n",
    "\n",
    "# predict the value\n",
    "y_pred_sklearn = linear_reg_sklearn.predict(X)\n",
    "score = r2_score(y, y_pred_sklearn)\n",
    "print(\"=\"*100)\n",
    "print(\"R2 score of the model is {}\".format(score))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "R2 score of the model is 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Polynomial Ridge Regression from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class PolynamialRidgeRegression(Regression):\n",
    "    \"\"\"\n",
    "    Polynomail Ridge Regression is basically polynomial regression with l2 regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, iteration, degree, lamda):\n",
    "        \"\"\"\n",
    "        :param learning_rate: [range from 0 to infinity] the stpe distance used while doing gradiant decent.\n",
    "        :param iteration: int - Number of iteration to do.\n",
    "        :param degree: int - The degree of the polynomial that the feature transformed to.\n",
    "        \"\"\"\n",
    "        self.degree = degree\n",
    "        # No regularization here. So, making the regularization methods to return 0.\n",
    "        self.regularization = l2_regularization(lamda)\n",
    "        super().__init__(learning_rate, iteration, self.regularization)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # change the data to \n",
    "        X_poly = PolynomialFeature(X, degree=self.degree)\n",
    "        return super().train(X_poly, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        test_X_poly = PolynomialFeature(test_X, degree=self.degree)\n",
    "        return super().predict(test_X_poly)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#define the parameters\n",
    "param = {\n",
    "    \"lamda\": 0.1,\n",
    "    \"degree\" : 2,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"iteration\" : 100,\n",
    "}\n",
    "print(\"=\"*100)\n",
    "polynomial_reg = PolynamialRidgeRegression(**param)\n",
    "\n",
    "# Train the model.\n",
    "polynomial_reg.train(X, y) \n",
    "\n",
    "# Predict the values.\n",
    "y_pred = polynomial_reg.predict(X)\n",
    "\n",
    "#Root mean square error.\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "The Cost function for the iteration 10----->4178.872832133191 :)\n",
      "The Cost function for the iteration 20----->2887.989505020741 :)\n",
      "The Cost function for the iteration 30----->2785.6247039737964 :)\n",
      "The Cost function for the iteration 40----->2777.471815365709 :)\n",
      "The Cost function for the iteration 50----->2776.819294060092 :)\n",
      "The Cost function for the iteration 60----->2776.7666829082946 :)\n",
      "The Cost function for the iteration 70----->2776.7623662294877 :)\n",
      "The Cost function for the iteration 80----->2776.761991761519 :)\n",
      "The Cost function for the iteration 90----->2776.761953080877 :)\n",
      "The Cost function for the iteration 100----->2776.761947221511 :)\n",
      "The r2_score of the trained model 0.9718297887794873\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Machine Learning models scratch series....\n",
    "you can also check....\n",
    "\n",
    "- 1) Linear Regression         ---> https://www.kaggle.com/ninjaac/linear-regression-from-scratch\n",
    "- 2) Lasso Regression          ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 3) Ridge Regression          ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 4) ElasticNet Regression     ---> https://www.kaggle.com/ninjaac/elasticnet-regression \n",
    "- 5) Polynomail Regression     ---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression (Same Notebook you are looking now)\n",
    "- 5) PolynomailRidge Regression---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression (Same Notebook you are looking now)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}