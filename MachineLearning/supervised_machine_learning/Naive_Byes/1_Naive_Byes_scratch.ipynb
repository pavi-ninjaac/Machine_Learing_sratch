{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>t</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>t</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>t</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook  Temp Humidity Windy Play\n",
       "0      Rainy   Hot     High     f   no\n",
       "1      Rainy   Hot     High     t   no\n",
       "2   Overcast   Hot     High     f  yes\n",
       "3      Sunny  Mild     High     f  yes\n",
       "4      Sunny  Cool   Normal     f  yes\n",
       "5      Sunny  Cool   Normal     t   no\n",
       "6   Overcast  Cool   Normal     t  yes\n",
       "7      Rainy  Mild     High     f   no\n",
       "8      Rainy  Cool   Normal     f  yes\n",
       "9      Sunny  Mild   Normal     f  yes\n",
       "10     Rainy  Mild   Normal     t  yes\n",
       "11  Overcast  Mild     High     t  yes\n",
       "12  Overcast   Hot   Normal     f  yes\n",
       "13     Sunny  Mild     High     t   no"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"weather.csv\")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavieBayesClassifier:\n",
    "    \"\"\"\n",
    "    Class for Navie Bayes classifier.\n",
    "\n",
    "    Bayes Theorem:\n",
    "\t\t\t\t\t\t\t\t\t\tLikelihood * class prior probability\n",
    "\t\t\t\tPosterior Probability = -----------------------------------------\n",
    "\t\t\t\t\t\t\t\t\t\tPredictor prior probability (or) evidence\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t\t\t  \t\t\t P(x|y) * p(y)\n",
    "\t\t\t\t\t\t\t   P(y|x) = ------------------ \n",
    "\t\t\t\t\t\t\t\t\t\t\t  P(x)    \n",
    "    Naive Bayes classifier can be written as below:\n",
    " \t\t\t\t\t\t\t  \t\t\t P(x1, x2, ..., xn|y) * p(y)\n",
    "\t\t\t\t\t\t\t   P(y|x) = -----------------------------\n",
    "\t\t\t\t\t\t\t\t\t\t\t  P(x1,x2,...,xn)   \n",
    "    Because the conditional probabilities are independent to each other , it can be written as:\n",
    "\n",
    "  \t\t\t\t\t\t\t  \t\t\t P(x1|y)*P(x2|y)* ...* P(xn|y) * p(y)\n",
    "\t\t\t\t\t\t\t   P(y|x) = ------------------------------------\n",
    "\t\t\t\t\t\t\t\t\t\t\t  P(x1)*P(x2) * ...* P(xn)                                                \n",
    "    \"\"\"\n",
    "    # 1 | Find the likelihood.\n",
    "    # 2 | Find the prior probability.\n",
    "    # 3 | Find the evidence.\n",
    "    # 4 | And just do the calculation and get the prediction.\n",
    "\n",
    "    # Training:\n",
    "    # 1 | The training just gonna find the the likelihood,prior probability and evidence of all features and store it somewhere.\n",
    "\n",
    "    # Prediction:\n",
    "    # 1 | Just do the calculation using the trained probabilities and find the prediction value.\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X: DataFrame | None = None\n",
    "        self.y: Series | None = None\n",
    "        self.feature_names: list[str] = []\n",
    "        self.output_class: list[str] = []\n",
    "        self.training_size: int = 0\n",
    "        \n",
    "        self.prior_prob: dict[str, float] = {}\n",
    "        self.evidence: dict[str, dict[str, float]] = defaultdict(dict)\n",
    "        self.likelihood: dict = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    def _prior_probability(self):\n",
    "        \"\"\"\n",
    "        Calculate the prior probability for each output class. \n",
    "\n",
    "        prior probability:\n",
    "                            Number of times yes present in the dataset \n",
    "            p(y=yes) =  -----------------------------------------------\n",
    "                           Total size of the dataset.\n",
    "\n",
    "        Ex: output for the current dataset is:\n",
    "        {'yes': 0.64, 'no': 0.36}\n",
    "        \"\"\"\n",
    "        #for output_class in np.unique(y):\n",
    "        # 1 | Count the number of values in each class.\n",
    "        class_count: Series = self.y.value_counts()\n",
    "\n",
    "        # 2 | Store the prior probability.\n",
    "        for col in class_count.index:\n",
    "            self.prior_prob[col] = round(class_count[col] / self.training_size , 2)\n",
    "\n",
    "    def _likelihood(self):\n",
    "        \"\"\"\n",
    "        Find the likehood of each feature uniques elements for each output class.\n",
    "\n",
    "        P(x/y) == P(x/y=yes) (or it can be )  P(x/y=no) [in our example]\n",
    "        Find the conditional probability of each unique values.\n",
    "\n",
    "        EX: output looks something like this for out example:\n",
    "        {'Outlook': {'yes': {'Sunny': 0.43, 'Overcast': 0.29, 'Rainy': 0.29},\n",
    "                     'no': {'Rainy': 0.75, 'Sunny': 0.25}}), \n",
    "        'Temp': {'yes': {'Mild': 0.43, 'Cool': 0.43, 'Hot': 0.14},\n",
    "                 'no': {'Hot': 0.5, 'Cool': 0.25, 'Mild': 0.25}}),\n",
    "        'Humidity': {'yes': {'Normal': 0.71, 'High': 0.29},\n",
    "                     'no': {'High': 0.75, 'Normal': 0.25}}),\n",
    "        'Windy': {'yes': {'f': 0.71, 't': 0.29},\n",
    "                  'no': {'f': 0.5, 't': 0.5}})})\n",
    "\n",
    "        \"\"\"\n",
    "        output_class_count: Series = self.y.value_counts()\n",
    "        for output_class in output_class_count.index:\n",
    "            # We need to filter out the rows which has this output_class. \n",
    "            # Find the index of the rows which has this output_class.\n",
    "            output_class_index: list[int] = self.y[self.y == output_class].index\n",
    "            output_class_length: int = len(output_class_index)\n",
    "\n",
    "            for feature in self.feature_names:\n",
    "                # Find the conditional probability of each unique feature for the output class.\n",
    "                unique_val_count: Series = self.X[feature][output_class_index].value_counts()\n",
    "\n",
    "                for unique_val in unique_val_count.index:\n",
    "                    self.likelihood[feature][output_class][unique_val] = round(unique_val_count[unique_val] / output_class_length, 2)\n",
    "\n",
    "    def _evidence(self):\n",
    "        \"\"\" \n",
    "        Calculate the evidence.\n",
    "\n",
    "        P(x) = P(x1)*P(x2) * ...* P(xn)\n",
    "        P(x1) = P(data point) = Number of times the data point present in that particular feature / total size of the feature.\n",
    "\n",
    "        Ex: For our training dataset, it will look something like this.\n",
    "        defaultdict(<class 'dict'>, {'Outlook': {'Rainy': 0.45, 'Sunny': 0.36, 'Overcast': 0.18},\n",
    "                                     'Temp': {'Mild': 0.36, 'Cool': 0.36, 'Hot': 0.27},\n",
    "                                     'Humidity': {'Normal': 0.55, 'High': 0.45},\n",
    "                                     'Windy': {'f': 0.64, 't': 0.36}})\n",
    "        \"\"\"\n",
    "        for feature in self.feature_names:\n",
    "            # We have to find the unique values in each feature, and get the count of it.\n",
    "            unique_val_count: Series = self.X[feature].value_counts()\n",
    "            for unique_val in unique_val_count.index:\n",
    "                # Find the evidence of all uniques values in the feature.\n",
    "                self.evidence[feature][unique_val] = round(unique_val_count[unique_val] / self.training_size, 2)\n",
    "        \n",
    "    def fit(self, X: DataFrame, y: Series):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        :param X: training data feature values ---> N Dimentional.\n",
    "        :param y: training data target value -----> 1 Dimentional.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = self.X.columns\n",
    "        self.training_size = len(self.X)\n",
    "        self.output_class = np.unique(self.y)\n",
    "\n",
    "        self._prior_probability()\n",
    "        self._evidence()\n",
    "        self._likelihood()\n",
    "        \n",
    "    def predict(self, test_X: DataFrame):\n",
    "        \"\"\"\n",
    "        Calculate the posterior probability for the given data point.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for row_ind in range(len(test_X)):\n",
    "            output_class_prob: dict[str, float] = {}\n",
    "\n",
    "            row = test_X.iloc[row_ind, :] # Rainy\tHot\tHigh\tf\n",
    "\n",
    "            # Find the values for each output_class and assign the value has the higher probability as output.\n",
    "            for output_class in self.output_class:\n",
    "                # 1 | prior probability.\n",
    "                prior_prob = self.prior_prob[output_class]\n",
    "\n",
    "                evidence = 1\n",
    "                likelihood = 1\n",
    "                for feature, fea_val in zip(self.feature_names, row):\n",
    "                    # # 2 | Evidence:\n",
    "                    if fea_val not in self.evidence[feature]:\n",
    "                        # If we don't have the particular data to find the evidence means that will be (0/size):\n",
    "                        evidence *= 0\n",
    "                    else:   \n",
    "                        evidence *= self.evidence[feature][fea_val]\n",
    "\n",
    "                    # 3 | likelikood:\n",
    "                    if fea_val not in self.likelihood[feature][output_class]:\n",
    "                        # When there is no value \n",
    "                        likelihood *= 0\n",
    "                    else:\n",
    "                        likelihood *= self.likelihood[feature][output_class][fea_val]\n",
    "                        \n",
    "                # find the posterior probability:\n",
    "                prob = (likelihood * prior_prob) / evidence\n",
    "                output_class_prob[output_class] = prob\n",
    "\n",
    "            # Get the maximum out of this values and assign that as output.\n",
    "            y_pred.append(max(output_class_prob, key = lambda x: output_class_prob[x]))\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset we have ----> 14\n",
      "Spiliting the values ...\n",
      "Length of the training dataset we have ----> 11\n",
      "Length of the testing dataset we have ----> 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook  Temp Humidity Windy\n",
       "0      Rainy   Hot     High     f\n",
       "1      Rainy   Hot     High     t\n",
       "2   Overcast   Hot     High     f\n",
       "3      Sunny  Mild     High     f\n",
       "4      Sunny  Cool   Normal     f\n",
       "5      Sunny  Cool   Normal     t\n",
       "6   Overcast  Cool   Normal     t\n",
       "7      Rainy  Mild     High     f\n",
       "8      Rainy  Cool   Normal     f\n",
       "9      Sunny  Mild   Normal     f\n",
       "10     Rainy  Mild   Normal     t"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training and test data.\n",
    "print(f\"Length of the dataset we have ----> {len(df)}\")\n",
    "print(\"Spiliting the values ...\")\n",
    "cutoff_point = int(len(df) * 0.8)\n",
    "train, test = df[:cutoff_point], df[cutoff_point:]\n",
    "print(f\"Length of the training dataset we have ----> {len(train)}\")\n",
    "print(f\"Length of the testing dataset we have ----> {len(test)}\")\n",
    "\n",
    "X, y = train.drop(df.columns[-1], axis=1), train[df.columns[-1]]\n",
    "test_X = test.drop(df.columns[-1], axis=1)\n",
    "test_y = test[df.columns[-1]]\n",
    "X.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      no\n",
       "1      no\n",
       "2     yes\n",
       "3     yes\n",
       "4     yes\n",
       "5      no\n",
       "6     yes\n",
       "7      no\n",
       "8     yes\n",
       "9     yes\n",
       "10    yes\n",
       "Name: Play, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values....\n",
      "11    yes\n",
      "12    yes\n",
      "13     no\n",
      "Name: Play, dtype: object\n",
      "Predicted values....\n",
      "['yes', 'yes', 'yes']\n"
     ]
    }
   ],
   "source": [
    "naive_byes_classifier = NavieBayesClassifier()\n",
    "naive_byes_classifier.fit(X, y)\n",
    "pred_y = naive_byes_classifier.predict(test_X)\n",
    "\n",
    "# Compare the actual and predicted output values.\n",
    "print(\"Actual values....\")\n",
    "print(test_y)\n",
    "\n",
    "print(\"Predicted values....\")\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prediction is not so bad. \n",
    "\n",
    "# we can predict for something else also.Rainy\tMild\tHigh\tf\n",
    "d = DataFrame([{\"Outlook\":\"Rainy\",\t\"Temp\":\"Mild\",\t\"Humidity\":\"High\",\t\"Windy\":\"f\"}])\n",
    "naive_byes_classifier.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from internet:\n",
    "----------------------\n",
    "For understanding the math:\n",
    "#Likelihood Table\n",
    "#Outlook\n",
    "Play Overcast Rainy Sunny \n",
    "Yes  4/9      2/9   3/9\n",
    "No   0/5      3/5   2/5\n",
    "     ___      ___   ___\n",
    "     4/14     5/14  5/14\n",
    "#Temp\n",
    "Play  Cool  Mild  Hot\n",
    "Yes   3/9   4/9   2/9\n",
    "No    1/5   2/5   2/5\n",
    "      ___   ___   ___\n",
    "      4/14  6/14  4/14\n",
    "#Humidity \n",
    "Play  High  Normal\n",
    "Yes   3/9   6/9\n",
    "No    4/5   1/5\n",
    "      ___   ___  \n",
    "      7/14  7/14 \n",
    "#Windy\n",
    "Play   f     t\n",
    "Yes    6/9   3/9\n",
    "No     2/5   3/5\n",
    "       ___   ___ \n",
    "       8/14  6/14  \n",
    "\n",
    "\n",
    "P(y=Yes|x) = P(Yes|Rainy,Mild,Normal,t)\n",
    "    P(Rainy,Mild,Normal,t|Yes) * P(Yes)\n",
    "= ___________________________________\n",
    "        P(Rainy,Mild,Normal,t)\n",
    "    P(Rainy|Yes)*P(Mild|Yes)*P(Normal|Yes)*P(t|Yes)*P(Yes)\n",
    "= ______________________________________________________\n",
    "            P(Rainy)*P(Mild)*P(Normal)*P(t)\n",
    "\n",
    "\n",
    "    (2/9) * (4/9) * (6/9) * (3/9) * (9/14)\n",
    "= _______________________________________\n",
    "    (5/14) * (6/14) * (7/14) * (6/14)\n",
    "\n",
    "= 0.43 \n",
    "\n",
    "P(y=No|x) = P(No|Rainy,Mild,Normal,t)\n",
    "    P(Rainy,Mild,Normal,t|No) * P(No)\n",
    "= ___________________________________\n",
    "        P(Rainy,Mild,Normal,t)\n",
    "    P(Rainy|No)*P(Mild|No)*P(Normal|No)*P(t|No)*P(No)\n",
    "= ______________________________________________________\n",
    "            P(Rainy)*P(Mild)*P(Normal)*P(t)\n",
    "    (3/5) * (2/5) * (1/5) * (3/5) * (5/14)\n",
    "= _______________________________________\n",
    "    (5/14) * (6/14) * (7/14) * (6/14)\n",
    "\n",
    "= 0.31\n",
    "Now, P(Play=Yes|Rainy,Mild,Normal,t) has the highest Posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference bwt aive Bayes classifier and Bayes classifier?\n",
    "\n",
    "The Naive Bayes classifier and the Bayes classifier both use Bayes' theorem to make predictions, but they differ in their assumptions and implementation.\n",
    "\n",
    "The Bayes classifier is a probabilistic model that calculates the probability of a hypothesis given the evidence. It considers all possible combinations of features and their joint probabilities. However, in practice, it can be computationally expensive and difficult to implement for complex problems due to the need to estimate the joint probability distribution of all features.\n",
    "\n",
    "On the other hand, the Naive Bayes classifier makes a \"naive\" assumption that all features are independent of each other given the class label. This simplifies the calculation of probabilities, making it computationally efficient and easier to implement. However, this assumption may not hold true in real-world scenarios, leading to potential inaccuracies in predictions.\n",
    "\n",
    "In summary, the main difference lies in the assumption of features independence. The Naive Bayes classifier assumes featureas are `conditionally independence`, while the Bayes classifier does not make this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naïve Bayes (GaussianNB): This is a variant of the Naïve Bayes classifier, which is used with Gaussian distributions—i.e. normal distributions—and continuous variables. This model is fitted by finding the mean and standard deviation of each class. \n",
    "\n",
    "- Multinomial Naïve Bayes (MultinomialNB): This type of Naïve Bayes classifier assumes that the features are from multinomial distributions. This variant is useful when using discrete data, such as frequency counts, and it is typically applied within natural language processing use cases, like spam classification. \n",
    "\n",
    " - Bernoulli Naïve Bayes (BernoulliNB): This is another variant of the Naïve Bayes classifier, which is used with Boolean variables—that is, variables with two values, such as True and False or 1 and 0. \n",
    "\n",
    " - CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index \n",
    ", has its own categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Counter({1: 0.052631578947368425})\n",
      "1 2\n",
      "Counter({1: 0.052631578947368425, 2: 0.052631578947368425})\n",
      "2 2\n",
      "Counter({2: 0.3157894736842105, 1: 0.052631578947368425})\n",
      "3 1\n",
      "Counter({1: 0.5789473684210527, 2: 0.3157894736842105})\n",
      "4 2\n",
      "Counter({1: 0.5789473684210527, 2: 0.42105263157894735})\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "k_nearest_labels = [1,2,2,1,2]\n",
    "k_nearest_distances = [10, 10, 2, 1 ,5]\n",
    "weight_sum = sum((1/d) for d in k_nearest_distances)\n",
    "weighted_votes = Counter()\n",
    "for i, label in enumerate(k_nearest_labels):\n",
    "    print(i, label)\n",
    "    print(weighted_votes)\n",
    "print(weighted_votes.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
