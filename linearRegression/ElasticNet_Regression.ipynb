{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common Regression Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Regression:\n",
    "    def __init__(self, learning_rate, iteration, regularization):\n",
    "        \"\"\"\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model.\n",
    "        self.lr = learning_rate\n",
    "        self.it = iteration\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        :param y: Original target value.\n",
    "        :param y_pred: predicted target value.\n",
    "        \"\"\"\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "    \n",
    "    def hypothesis(self, weights, bias, X):\n",
    "        \"\"\"\n",
    "        :param weights: parameter value weight.\n",
    "        :param X: Training samples.\n",
    "        \"\"\"\n",
    "        return np.dot(X, weights) #+ bias\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Target value should be in the shape of (n, 1) not (n, ).\n",
    "        # So, this will check that and change the shape to (n, 1), if not.\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            # we need to change it to the 1 D array, not a list.\n",
    "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
    "            return \n",
    "        \n",
    "        # m is the number of training samples.\n",
    "        self.m = X.shape[0]\n",
    "        # n is the number of features.\n",
    "        self.n = X.shape[1]\n",
    "\n",
    "        # Set the initial weight.\n",
    "        self.w = np.zeros((self.n , 1))\n",
    "\n",
    "        # bias.\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.it+1):\n",
    "            # 1. Find the predicted value through the hypothesis.\n",
    "            # 2. Find the Cost function value.\n",
    "            # 3. Find the derivation of weights.\n",
    "            # 4. Apply Gradient Decent.\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            #print(\"iteration\",it)\n",
    "            #print(\"y predict value\",y_pred)\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            #print(\"Cost function\",cost)\n",
    "            # fin the derivative.\n",
    "            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "            #print(\"weights derivation\",dw)\n",
    "            #db = -(2 / self.m) * np.sum((y_pred - y))\n",
    "\n",
    "            # change the weight parameter.\n",
    "            self.w = self.w - self.lr * dw\n",
    "            #print(\"updated weights\",self.w)\n",
    "            #self.b = self.b - self.lr * db\n",
    "\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization class for elastic net"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class l1_l2_regularization:\n",
    "    def __init__(self, lamda = 0.1, l_ratio = 0.5):\n",
    "        self.lamda = lamda \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        l1_contribution = self.l_ratio * self.lamda * np.sum(np.abs(weights))\n",
    "        l2_contribution = (1 - self.l_ratio) * self.lamda * 0.5 * np.sum(np.square(weights))\n",
    "        return (l1_contribution + l2_contribution)\n",
    "\n",
    "    def derivation(self, weights):\n",
    "        l1_derivation = self.lamda * self.l_ratio * np.sign(weights)\n",
    "        l2_derivation = self.lamda * (1 - self.l_ratio) * weights\n",
    "        return (l1_derivation + l2_derivation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Define the traning data.\n",
    "X, y = make_regression(n_samples=50000, n_features=8)\n",
    "\n",
    "# Chnage the shape of the target to 1 dimentional array.\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 50000\n",
      "Number of training features --------> 8\n",
      "Shape of the target value ----------> (50000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# display the data.\n",
    "data = pd.DataFrame(X)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.867789 -0.101748 -1.880689  0.297810  2.738893 -0.505242  0.434876   \n",
       "1 -0.770503 -0.931669  1.308884  0.389382  1.495710 -0.105214 -0.000994   \n",
       "2 -1.400998 -0.615129 -0.454939 -0.463922  0.726401 -0.275039 -0.093223   \n",
       "3  0.225891 -0.845367  0.574860  1.833177 -1.292813  0.846228 -1.296938   \n",
       "4 -0.558011  0.452136 -1.667938 -0.677074  0.493884 -0.087129  0.007175   \n",
       "\n",
       "          7  \n",
       "0 -0.559950  \n",
       "1 -0.002724  \n",
       "2  2.472022  \n",
       "3 -1.717160  \n",
       "4 -0.688758  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.867789</td>\n",
       "      <td>-0.101748</td>\n",
       "      <td>-1.880689</td>\n",
       "      <td>0.297810</td>\n",
       "      <td>2.738893</td>\n",
       "      <td>-0.505242</td>\n",
       "      <td>0.434876</td>\n",
       "      <td>-0.559950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.770503</td>\n",
       "      <td>-0.931669</td>\n",
       "      <td>1.308884</td>\n",
       "      <td>0.389382</td>\n",
       "      <td>1.495710</td>\n",
       "      <td>-0.105214</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.002724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400998</td>\n",
       "      <td>-0.615129</td>\n",
       "      <td>-0.454939</td>\n",
       "      <td>-0.463922</td>\n",
       "      <td>0.726401</td>\n",
       "      <td>-0.275039</td>\n",
       "      <td>-0.093223</td>\n",
       "      <td>2.472022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225891</td>\n",
       "      <td>-0.845367</td>\n",
       "      <td>0.574860</td>\n",
       "      <td>1.833177</td>\n",
       "      <td>-1.292813</td>\n",
       "      <td>0.846228</td>\n",
       "      <td>-1.296938</td>\n",
       "      <td>-1.717160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.558011</td>\n",
       "      <td>0.452136</td>\n",
       "      <td>-1.667938</td>\n",
       "      <td>-0.677074</td>\n",
       "      <td>0.493884</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>-0.688758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# display the data.\n",
    "data_y = pd.DataFrame(y)\n",
    "data_y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            0\n",
       "0  131.922335\n",
       "1   60.788398\n",
       "2   83.642388\n",
       "3 -212.055816\n",
       "4 -104.694715"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131.922335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.788398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.642388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-212.055816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-104.694715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Elastic net from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Elastica Net is the combination of Lasso and Ridge regression. It has the advanage of both Lasso and Ridge Regression.\n",
    "- **Lasso** --> Will completely eliminates the correlated/unwanted features from model. So, the model has less complexity and becomes easy to learn.\n",
    "- **Ridge** --> Will shrink the parameter close to zero , but never eliminate the variables.\n",
    "\n",
    "Usage:\n",
    "- We can use the **Lasso** model --> when we have a dataset with more correlated/unwanted features.\n",
    "- We can use the **Ridge** model --> When we have a dataset with more useful features.\n",
    "\n",
    "### We can use the Elastic Net when we don't know the correlation between the features and having lots of (1000s<) features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class ElasticNetRegression(Regression):\n",
    "    \"\"\"\n",
    "    Elastic Regression class\n",
    "    \"\"\"\n",
    "    def __init__(self, lamda, l_ratio, learning_rate, iteration):\n",
    "        \"\"\"\n",
    "        Define the hyperparameters we are going to use in this model.\n",
    "        :param lamda: Regularization factor.\n",
    "        :param l_ratio: The ratio between lasso and ridge regression--> default is 0.5.\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.regularization = l1_l2_regularization(lamda,l_ratio)\n",
    "        super(ElasticNetRegression, self).__init__(learning_rate, iteration, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        return super(ElasticNetRegression, self).train(X, y)\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        parma test_X: Value need to be predicted.\n",
    "        \"\"\"\n",
    "        return super(ElasticNetRegression, self).predict(test_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#define the parameters\n",
    "param = {\n",
    "    \"l_ratio\" : 0.5,\n",
    "    \"lamda\" : 0.1,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"iteration\" : 100\n",
    "}\n",
    "print(\"=\"*100)\n",
    "elastic_net_reg = ElasticNetRegression(**param)\n",
    "\n",
    "# Train the model.\n",
    "elastic_net_reg.train(X, y) \n",
    "\n",
    "# Predict the values.\n",
    "y_pred = elastic_net_reg.predict(X)\n",
    "\n",
    "#Root mean square error.\n",
    "score = r2_score(y, y_pred)\n",
    "print(\"The r2_score of the trained model\", score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "The Cost function for the iteration 10----->1976.4900563834374 :)\n",
      "The Cost function for the iteration 20----->701.1758775240576 :)\n",
      "The Cost function for the iteration 30----->563.2582248687778 :)\n",
      "The Cost function for the iteration 40----->548.3351232529961 :)\n",
      "The Cost function for the iteration 50----->546.7197090314678 :)\n",
      "The Cost function for the iteration 60----->546.5445700049715 :)\n",
      "The Cost function for the iteration 70----->546.5256016500784 :)\n",
      "The Cost function for the iteration 80----->546.5236497045239 :)\n",
      "The Cost function for the iteration 90----->546.523290811168 :)\n",
      "The Cost function for the iteration 100----->546.5233321859182 :)\n",
      "The r2_score of the trained model 0.9976669791861328\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Elastic Net using scikit-learn for comparition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# data is already defined, going to use the same data for comparision.\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 50000\n",
      "Number of training features --------> 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "elastic_net_sklearn = ElasticNet()\n",
    "elastic_net_sklearn.fit(X, y)\n",
    "\n",
    "# predict the value\n",
    "y_pred_sklearn = elastic_net_sklearn.predict(X)\n",
    "score = r2_score(y, y_pred_sklearn)\n",
    "print(\"=\"*100)\n",
    "print(\"R2 score of the model is {}\".format(score))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "R2 score of the model is 0.8855865841875906\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model perofrming well with the default parameters. We can tune the parameter littele bit to get some more good results without the tension of pverfitting :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Machine Learning models scratch series....\n",
    "you can also check....\n",
    "\n",
    "- 1) Linear Regression     ---> https://www.kaggle.com/ninjaac/linear-regression-from-scratch\n",
    "- 2) Lasso Regression      ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 3) Ridge Regression      ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 4) ElasticNet Regression ---> https://www.kaggle.com/ninjaac/elasticnet-regression (Same Notebook you are looking now)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}